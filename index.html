<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Mamshad Nayeem Rizve</title>
  
  <meta name="author" content="Mamshad Nayeem Rizve">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  
<link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/seal_icon.png">
  <link rel   = "stylesheet" href    ="https://use.fontawesome.com/releases/v5.0.7/css/all.css">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center">
                <name>Mamshad Nayeem Rizve</name>
              </p>
              <p>I am a PhD candidate at <a href="https://www.crcv.ucf.edu/">Center for Research in Computer Vision, UCF</a>, where I am advised by <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Prof. Mubarak Shah</a>. I'm interested in computer vision, and machine learning. My research primarily focuses on learning with limited labels, which mostly includes semi-supervised learning, few-shot learning, and self-supervised learning. I have also worked on activity detection, temporal action localization, learning with noisy labels, and multi-modal learning.
              </p>
              <p style="text-align:center">
                <a href="mailto:nayeemrizve@gmail.com">Email</a> &nbsp/&nbsp
                <a href="data/CV_Mamshad.pdf">CV</a> &nbsp/&nbsp
                <a href="https://scholar.google.com/citations?user=kA8ZM5oAAAAJ&hl">Google Scholar</a> &nbsp/&nbsp
                <a href="https://twitter.com/MamshadR">Twitter</a> &nbsp/&nbsp
                <a href="https://www.linkedin.com/in/mamshad-nayeem-rizve/">LinkedIn</a> &nbsp/&nbsp
                <a href="https://github.com/nayeemrizve">GitHub</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/rizve_circle.png"><img style="width:100%;max-width:100%" alt="profile photo" src="images/rizve_circle.png" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>
	
	<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
            <td style="padding:20px;width:100%;vertical-align:middle">
              <heading>Updates</heading>
              <p>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> February 2023: Two papers accepted at CVPR 2023<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> October 2022: Patent for real-time spatio-temporal activity detection from untrimmed video has been granted<br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> July 2022: Our realistic semi-supervised learning paper has been selected for oral presentation at ECCV 2022<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> July 2022: Two papers accepted at ECCV 2022<br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> May 2022: Started summer internship with the Decision AI team at Microsoft<br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> March 2022: Paper accepted at CVPR 2022<br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> May 2021: Started summer internship with the Perception team at Aurora<br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> March 2021: Paper accepted at CVPR 2021<br>
                &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> January 2021: Paper accepted at ICLR 2021<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> January 2021: Our Gabriella paper has been awarded the best scientific paper award at ICPR 2020<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> October 2020: Paper accepted at ICPR 2020<br>
	        &nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> June 2020: Placed first at ActEV SDL Challenge (ActivityNet workshop at CVPR 2020)<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> October 2019: Placed second at the TRECVID leaderboard<br>
		&nbsp <i class="fa fa-share-alt" style="font-size:12px"></i> August 2018: Started PhD at CRCV, UCF<br> 
              </p>
            </td>
          </tr>
        </tbody></table>
	      
    <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
             <tr>
             <td style="padding:20px;width:100%;vertical-align:middle"> 
               <heading>Publications</heading>
            </td>
        </tr> 
    </tbody></table>  

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
		
	<tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/timebalance_cvpr2023.png" alt="timebalance-cvpr2023" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2303.16268" id="MCG_journal">
                <papertitle>TimeBalance: Temporally-Invariant and Temporally-Distinctive Video Representations for Semi-Supervised Action Recognition</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/ishan-dave-crcv">Ishan Dave</a>, <strong>Mamshad Nayeem Rizve</strong>, <a href="https://www.crcv.ucf.edu/chenchen/">Chen Chen</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>CVPR</em>, 2023
              <br>
              <a href="https://arxiv.org/abs/2303.16268">arxiv</a>/
              <a href="data/timebalance_cvpr2023.bib">bibtex</a> /
              <a href="https://github.com/DAVEISHAN/TimeBalance">code</a>
              <p>
		   We propose a student-teacher semi-supervised learning framework, where we distill knowledge from a temporally-invariant and temporally-distinctive teacher. Depending on the nature of the unlabeled video, we dynamically combine the knowledge of these two teachers based on a novel temporal similarity-based reweighting scheme.
		</p>
            </td>
          </tr>
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/realistic_eccv2022.png" alt="realistic-eccv2022" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2207.02269" id="MCG_journal">
                <papertitle>Towards Realistic Semi-Supervised Learning</papertitle>
              </a>
              <br>
              <strong>Mamshad Nayeem Rizve</strong>, <a href="https://www.linkedin.com/in/navid-kardan-b2630a88/">Navid Kardan</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>ECCV</em>, 2022 <font color="red"><strong>(Oral Presentation, 2.7% acceptance rate)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2207.02269">arxiv</a> /
              <a href="data/realistic_eccv2022.bib">bibtex</a> /
              <a href="https://github.com/nayeemrizve/TRSSL">code</a>
	      <p></p>
              <p>
		We propose a new method for open-world semi-supervised learning that utilizes sample uncertainty and incorporates prior knowledge about class distribution to generate reliable class-distribution-aware pseudo-labels for samples belonging to both known and unknown classes.
	      </p>
		    
            </td>
          </tr>
		
		
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/openldn_eccv2022.png" alt="openldn-eccv2022" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2207.02261" id="MCG_journal">
                <papertitle>OpenLDN: Learning to Discover Novel Classes for Open-World Semi-Supervised Learning</papertitle>
              </a>
              <br>
              <strong>Mamshad Nayeem Rizve</strong>, <a href="https://www.linkedin.com/in/navid-kardan-b2630a88/">Navid Kardan</a>, <a href="https://salman-h-khan.github.io/">Salman Khan</a>, <a href="https://sites.google.com/view/fahadkhans/home">Fahad Shahbaz Khan</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>ECCV</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2207.02261">arxiv</a> /
              <a href="data/openldn_eccv2022.bib">bibtex</a> /
              <a href="https://github.com/nayeemrizve/OpenLDN">code</a>
              <p>
		    OpenLDN utilizes a pairwise similarity loss with bi-level optimization to discover novel classes and transforms the open-world SSL problem into a standard SSL problem, outperforming current state-of-the-art methods with better accuracy/training time trade-off.
		</p>
            </td>
          </tr>
		
		
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/unicon_cvpr2022.png" alt="invariance-equivariance-cvpr2021" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2203.14542" id="MCG_journal">
                <papertitle>UNICON: Combating Label Noise Through Uniform Selection and Contrastive Learning</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/nazmul-karim-1b5805115/">Nazmul Karim</a>, <strong>Mamshad Nayeem Rizve</strong>, <a href="https://www.ece.ucf.edu/person/nazanin-rahnavard/">Nazanin Rahnavard</a>, <a href="https://ajmalsaeed.net/">Ajmal Mian</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>CVPR</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2203.14542">arxiv</a> /
              <a href="data/unicon_cvpr2022.bib">bibtex</a> /
              <a href="https://github.com/nazmul-karim170/unicon-noisy-label">code</a>
              <p>
		UNICON is a robust sample selection approach for training with high label noise. It incorporates a Jensen-Shannon divergence based uniform sample selection mechanism and contrastive learning.
	</p>
            </td>
          </tr>
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/tclr_cviu2022.png" alt="invariance-equivariance-cvpr2021" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2101.07974" id="MCG_journal">
                <papertitle>TCLR: Temporal contrastive learning for video representation</papertitle>
              </a>
              <br>
              <a href="https://www.linkedin.com/in/ishan-dave-crcv">Ishan Dave</a>, <a href="https://www.linkedin.com/in/rohitguptahpf">Rohit Gupta</a>, <strong>Mamshad Nayeem Rizve</strong>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>CVIU</em>, 2022
              <br>
              <a href="https://arxiv.org/abs/2101.07974">arxiv</a> /
              <a href="https://www.sciencedirect.com/science/article/pii/S1077314222000376">elsevier</a> /
              <a href="data/tclr_cviu2022.bib">bibtex</a> /
              <a href="https://github.com/DAVEISHAN/TCLR">code</a> /
              <a href="https://www.youtube.com/watch?v=NfE1CXqzE8s&start=2212">video</a>
              <p>We propose a new temporal contrastive learning framework for self-supervised video representation learning, consisting of two novel losses that aim to increase the temporal diversity of learned features. The framework achieves state-of-the-art results on various downstream video understanding tasks, including significant improvement in fine-grained action classification for visually similar classes.</p>
            </td>
          </tr>
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/ier_cvpr2021.png" alt="invariance-equivariance-cvpr2021" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2103.01315" id="MCG_journal">
                <papertitle>Exploring Complementary Strengths of Invariant and Equivariant Representations for Few-Shot Learning</papertitle>
              </a>
              <br>
              <strong>Mamshad Nayeem Rizve</strong>, <a href="https://salman-h-khan.github.io/">Salman Khan</a>, <a href="https://sites.google.com/view/fahadkhans/home">Fahad Shahbaz Khan</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>CVPR</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2103.01315">arxiv</a> /
              <a href="data/ier_cvp2021.bib">bibtex</a> /
              <a href="https://github.com/nayeemrizve/invariance-equivariance">code</a> /
              <a href="https://www.youtube.com/watch?v=NfE1CXqzE8s&start=1100">video</a>
              <p>
		    We propose a novel training mechanism for few-shot learning that simultaneously enforces equivariance and invariance to geometric transformations, allowing the model to learn features that generalize well to novel classes with few samples. 
		    </p>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/ups_iclr2021.png" alt="ups-iclr2021" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:35px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2101.06329" id="MCG_journal">
                <papertitle>In Defense of Pseudo-Labeling: An Uncertainty-Aware Pseudo-label Selection Framework for Semi-Supervised Learning</papertitle>
              </a>
              <br>
              <strong>Mamshad Nayeem Rizve</strong>, <a href="https://www.linkedin.com/in/kevin-duarte-vision/">Kevin Duarte</a>, <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>ICLR</em>, 2021
              <br>
              <a href="https://arxiv.org/abs/2101.06329">arxiv</a> /
              <a href="https://openreview.net/forum?id=-ODN6SbiUU">openreview</a> /
              <a href="data/ups_iclr2021.bib">bibtex</a> /
              <a href="https://github.com/nayeemrizve/ups">code</a> /
              <a href="https://www.youtube.com/watch?v=NfE1CXqzE8s&start=20">video</a>
              <p>We propose an uncertainty-aware pseudo-label selection (UPS) framework that reduces pseudo-label noise encountered during training, and allows for the creation of negative pseudo-labels for multi-label classification and negative learning. </p>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/gabriella_icpr2020.png" alt="gabriella-icpr2020" width="160" style="border-style: none">
            </td>
            <td style="padding-bottom:10px;width:75%;vertical-align:middle">
              <a href="https://arxiv.org/abs/2004.11475" id="MCG_journal">
                <papertitle>Gabriella: An Online System for Real-Time Activity Detection in Untrimmed Security Videos</papertitle>
              </a>
              <br>
              <strong>Mamshad Nayeem Rizve</strong>, <a href="https://www.linkedin.com/in/demiru/">Ugur Demir</a>, <a href="https://www.linkedin.com/in/praveen-tirupattur-2044ba51/">Praveen Tirupattur</a>, <a href="https://aayushjr.github.io/">Aayush Jung Rana</a>, <a href="https://www.linkedin.com/in/kevin-duarte-vision/">Kevin Duarte</a>, <a href="https://www.linkedin.com/in/ishan-dave-crcv">Ishan Dave</a>, <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>
              <br>
              <em>ICPR</em>, 2020 <font color="red"><strong>(Best Paper Award)</strong></font>
              <br>
              <a href="https://arxiv.org/abs/2004.11475">arxiv</a> /
              <a href="https://www.crcv.ucf.edu/research/projects/gabriella-an-online-system-for-real-time-activity-detection-in-untrimmed-security-videos/">project page</a> /
              <a href="data/gabriella_icpr2020.bib">bibtex</a> /
              <a href="https://www.crcv.ucf.edu/wp-content/uploads/2020/08/Projects_Gabriella_Slides.pdf">slides</a> /
              <a href="https://www.youtube.com/watch?v=O64331jZczo">video</a>
              <p>Gabbriella consists of three stages: tubelet extraction, activity classification, and online tubelet merging. Gabriella utilizes a localization network for tubelet extraction, with a novel Patch-Dice loss to handle variations in actor size, and a Tubelet-Merge Action-Split (TMAS) algorithm to detect activities efficiently and robustly.</p>
            </td>
          </tr>
					

        </tbody></table>
	      
	      <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
             <tr>
             <td style="padding:20px;width:100%;vertical-align:middle"> 
               <heading>Patents</heading>
            </td>
        </tr> 
    </tbody></table>  

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:10px;width:25%;vertical-align:top">
              <img src="images/gabriella_patent.png" alt="gabriella-patent" width="160" style="border-style: none">
            </td>
            <td width="75%" valign="middle">
              <a href="https://patentimages.storage.googleapis.com/1a/1e/ec/af681095ca1677/US20220222940A1.pdf" id="MCG_journal">
                <papertitle>Methods of Real-Time Spatio-Temporal Activity Detection and Categorization from Untrimmed Video Segments</papertitle>
              </a>
              <br>
              <a href="https://www.crcv.ucf.edu/person/rawat/">Yogesh S Rawat</a>, <a href="https://www.crcv.ucf.edu/person/mubarak-shah/">Mubarak Shah</a>, <a href="https://aayushjr.github.io/">Aayush Jung Rana</a>, <a href="https://www.linkedin.com/in/praveen-tirupattur-2044ba51/">Praveen Tirupattur</a>, <strong>Mamshad Nayeem Rizve</strong>
              <br>
              US Patent 11468676
              <br>
              <a href="https://patentimages.storage.googleapis.com/1a/1e/ec/af681095ca1677/US20220222940A1.pdf">Details</a>
              <p></p>
            </td>
          </tr>

	</tbody></table> 			
        

  </table>
</body>

</html>
